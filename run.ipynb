{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ordinary-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from easydict import EasyDict\n",
    "\n",
    "import h5py\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch2trt import torch2trt\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "from alphapose.models import builder\n",
    "from alphapose.utils.config import update_config\n",
    "from alphapose.utils.transforms import get_func_heatmap_to_coord\n",
    "from alphapose.utils.bbox import (_box_to_center_scale, _center_scale_to_box)\n",
    "from alphapose.utils.transforms import (get_affine_transform, im_to_torch)\n",
    "from detector.yolo4_csp.ScaledYOLOv4.utils.torch_utils import time_synchronized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "equipped-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./detector/')\n",
    "sys.path.append('./detector/yolo4_csp/ScaledYOLOv4/')\n",
    "sys.path.append('./detector/yolo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consistent-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detector.yolo4_api import YOLO4Detector\n",
    "from detector.yolo4trt_api import YOLO4TRTDetector\n",
    "from detector.yolo4_cfg import cfg as yolo_cfg\n",
    "from detector.yolo4_csp.preprocess import prep_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "illegal-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(object):\n",
    "    \"\"\"Generation of cropped input person and pose heatmaps from SimplePose.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size: tuple\n",
    "        Input image size, as (height, width).\n",
    "    output_size: tuple\n",
    "        Heatmap size, as (height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,input_size, output_size, gpu_device='cuda'):\n",
    "        self._gpu_device = gpu_device\n",
    "\n",
    "        self._input_size = input_size\n",
    "        self._heatmap_size = output_size\n",
    "\n",
    "        self._aspect_ratio = float(input_size[1]) / input_size[0]  # w / h\n",
    "        \n",
    "    def preprocess(self, src, bbox):\n",
    "        x, y, w, h = bbox\n",
    "        center, scale = _box_to_center_scale(\n",
    "            x, y, w, h, self._aspect_ratio)\n",
    "        scale = scale * 1.0\n",
    "\n",
    "        input_size = self._input_size\n",
    "        inp_h, inp_w = input_size\n",
    "\n",
    "        trans = get_affine_transform(center, scale, 0, [inp_w, inp_h])\n",
    "        img = cv2.warpAffine(src, trans, (int(inp_w), int(inp_h)), flags=cv2.INTER_LINEAR)\n",
    "        bbox = _center_scale_to_box(center, scale)\n",
    "        \n",
    "        return img, bbox\n",
    "        \n",
    "\n",
    "    def transform(self, src, bbox):\n",
    "        x, y, w, h = bbox\n",
    "        center, scale = _box_to_center_scale(\n",
    "            x, y, w, h, self._aspect_ratio)\n",
    "        scale = scale * 1.0\n",
    "\n",
    "        input_size = self._input_size\n",
    "        inp_h, inp_w = input_size\n",
    "\n",
    "        trans = get_affine_transform(center, scale, 0, [inp_w, inp_h])\n",
    "        img = cv2.warpAffine(src, trans, (int(inp_w), int(inp_h)), flags=cv2.INTER_LINEAR)\n",
    "        bbox = _center_scale_to_box(center, scale)\n",
    "\n",
    "        img = im_to_torch(img)\n",
    "        img[0].add_(-0.406)\n",
    "        img[1].add_(-0.457)\n",
    "        img[2].add_(-0.480)\n",
    "\n",
    "        return img, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ordered-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_opt = EasyDict()\n",
    "yolo_opt.gpus = [i for i in range(torch.cuda.device_count())]\n",
    "yolo_opt.device = torch.device(\"cuda:\" + str(yolo_opt.gpus[0]) if yolo_opt.gpus[0] >= 0 else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "enhanced-railway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRTModule()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TRTModule()\n",
    "model.load_state_dict(torch.load('./pretrained_models/256x192_res50_lr1e-3_1x_reduced_TensorRT_FP16.pth'))\n",
    "model.eval()\n",
    "print('TensorRT network created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exact-moment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO4 model..\n",
      "TensorRT network created\n"
     ]
    }
   ],
   "source": [
    "detect = YOLO4TRTDetector(yolo_cfg, yolo_opt)\n",
    "detect.load_model_trt('./pretrained_models/YOLO4-CSP_TensorRT_FP16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "designing-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = update_config('./configs/human_reduced/resnet/256x192_res50_lr1e-3_1x_reduced_new.yaml')\n",
    "trans=Transform(cfg.DATA_PRESET.IMAGE_SIZE, cfg.DATA_PRESET.HEATMAP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "studied-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_to_coord = get_func_heatmap_to_coord(cfg)\n",
    "hm_size = cfg.DATA_PRESET.HEATMAP_SIZE\n",
    "norm_type = cfg.LOSS.get('NORM_TYPE', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dried-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfolder ='/private/home/taoshaf/src/c2f-vol-demo/data/h36m/images/'\n",
    "imgnames = os.listdir(imgfolder)\n",
    "imgnames =[imgname for imgname in imgnames if imgname.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "incorporate-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400/400] Average inference time: 0.026661021113395692 seconds\r"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "res = dict()\n",
    "with torch.no_grad():\n",
    "    for (i, imgname) in enumerate(imgnames[:400]):\n",
    "        imgfile = os.path.join(imgfolder, imgname)\n",
    "        img = cv2.imread(imgfile)\n",
    "        t1 = time_synchronized()\n",
    "        bbox = detect.detect_one_frame(0,img)\n",
    "        \n",
    "        if not bbox:\n",
    "            res[imgname[:-4]] = []\n",
    "            print(imgname[:-4])\n",
    "            continue\n",
    "        bbox = bbox[0]['bbox']\n",
    "        im, box = trans.transform(img[:,:,::-1], bbox)\n",
    "        im=im.unsqueeze(0).cuda()\n",
    "        output = model(im)\n",
    "        t2 = time_synchronized()\n",
    "        kpts = np.hstack(heatmap_to_coord(output[0],box,hm_shape=hm_size, norm_type=norm_type))\n",
    "        total += t2-t1\n",
    "        res[imgname[:-4]] = kpts.astype(np.float64)\n",
    "        if (i+1)%100 == 0:\n",
    "            print('[{}/{}] Average inference time: {} seconds'.format(i+1,i+1,total/(i+1)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-webster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
